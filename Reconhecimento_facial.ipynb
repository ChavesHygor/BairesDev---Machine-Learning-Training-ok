{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP4HE/KH3SCjmhMtaDTeQaj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChavesHygor/BairesDev---Machine-Learning-Training-ok/blob/main/Reconhecimento_facial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Rh8jBQ0biRS",
        "outputId": "86431924-1f75-4640-e2e2-eba4c7e1a901"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instalando bibliotecas: TensorFlow, OpenCV, MTCNN e Scikit-learn...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalação concluída!\n"
          ]
        }
      ],
      "source": [
        "# --- ETAPA 1: INSTALAÇÃO DAS BIBLIOTECAS ---\n",
        "\n",
        "# Instala as bibliotecas necessárias. Usamos 'headless' para o OpenCV em ambientes sem GUI como o Colab.\n",
        "print(\"Instalando bibliotecas: TensorFlow, OpenCV, MTCNN e Scikit-learn...\")\n",
        "!pip install -q tensorflow opencv-python-headless mtcnn scikit-learn\n",
        "print(\"Instalação concluída!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ETAPA 2: CONFIGURAÇÃO DO GOOGLE DRIVE E PASTAS ---\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Monta o Google Drive\n",
        "print(\"Montando o Google Drive...\")\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Define o caminho base para o nosso projeto\n",
        "base_dir = '/content/gdrive/My Drive/facial_recognition'\n",
        "\n",
        "# Cria a estrutura de pastas\n",
        "dataset_dir = os.path.join(base_dir, 'dataset')\n",
        "output_dir = os.path.join(base_dir, 'output')\n",
        "\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Estrutura de pastas criada em: '{base_dir}'\")\n",
        "print(\"\\nINSTRUÇÃO IMPORTANTE:\")\n",
        "print(f\"Agora, acesse a pasta '{dataset_dir}' no seu Google Drive.\")\n",
        "print(\"Dentro dela, crie uma pasta para cada pessoa que você quer reconhecer (ex: 'Albert Einstein', 'Marie Curie').\")\n",
        "print(\"Coloque várias fotos (5 a 10) de cada pessoa em sua respectiva pasta.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4PGBtlVbx2N",
        "outputId": "89e10fb5-434e-48f3-b7e1-5dc752a82dcb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Montando o Google Drive...\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Estrutura de pastas criada em: '/content/gdrive/My Drive/facial_recognition'\n",
            "\n",
            "INSTRUÇÃO IMPORTANTE:\n",
            "Agora, acesse a pasta '/content/gdrive/My Drive/facial_recognition/dataset' no seu Google Drive.\n",
            "Dentro dela, crie uma pasta para cada pessoa que você quer reconhecer (ex: 'Albert Einstein', 'Marie Curie').\n",
            "Coloque várias fotos (5 a 10) de cada pessoa em sua respectiva pasta.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ETAPA 3: DOWNLOAD E VERIFICAÇÃO DO MODELO FACENET (COM O LINK DO REPOSITÓRIO INDICADO) ---\n",
        "import os\n",
        "\n",
        "# Define os caminhos necessários\n",
        "base_dir = '/content/gdrive/My Drive/facial_recognition'\n",
        "output_dir = os.path.join(base_dir, 'output')\n",
        "os.makedirs(output_dir, exist_ok=True) # Garante que a pasta de saída existe\n",
        "facenet_model_path = os.path.join(output_dir, 'facenet_keras.h5')\n",
        "\n",
        "# 1. Deleta qualquer versão antiga/corrompida do arquivo\n",
        "print(\"Removendo qualquer arquivo de modelo antigo para garantir um download limpo...\")\n",
        "!rm -f \"{facenet_model_path}\"\n",
        "\n",
        "# 2. USA O LINK DE DOWNLOAD DIRETO DO REPOSITÓRIO GITHUB 'nyoki-mtl/keras-facenet'\n",
        "print(\"\\nBaixando o modelo FaceNet do repositório 'nyoki-mtl' (aprox. 95MB)...\")\n",
        "!wget --show-progress -O \"{facenet_model_path}\" \"https://github.com/D2KLab/Face-Celebrity-Recognition/raw/master/model/facenet_keras.h5\"\n",
        "\n",
        "# 3. VERIFICA O SUCESSO DO DOWNLOAD\n",
        "print(\"\\n--- Verificação Pós-Download ---\")\n",
        "# Verifica se o arquivo existe e tem um tamanho razoável (mais de 1MB)\n",
        "if os.path.exists(facenet_model_path) and os.path.getsize(facenet_model_path) > 1000000:\n",
        "    # 3A. Verifica o tamanho do arquivo (deve ser em torno de 95MB)\n",
        "    print(\"Verificando o tamanho do arquivo...\")\n",
        "    !ls -lh \"{facenet_model_path}\"\n",
        "\n",
        "    # 3B. Verifica o tipo do arquivo (deve ser 'Hierarchical Data Format (HDF) data')\n",
        "    print(\"\\nVerificando o tipo do arquivo...\")\n",
        "    !file \"{facenet_model_path}\"\n",
        "    print(\"\\nDOWNLOAD BEM-SUCEDIDO! Se o tamanho for ~95M e o tipo for 'HDF data', você pode prosseguir para a Célula 4.\")\n",
        "else:\n",
        "    print(\"\\nERRO CRÍTICO: O download falhou. O arquivo não foi criado ou está vazio.\")\n",
        "    print(\"Verifique sua conexão ou se o novo link ainda é válido.\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKnPgBlkb0Nt",
        "outputId": "8ec2b29e-45f5-4f9a-fb75-3ae29f426295"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removendo qualquer arquivo de modelo antigo para garantir um download limpo...\n",
            "\n",
            "Baixando o modelo FaceNet do repositório 'nyoki-mtl' (aprox. 95MB)...\n",
            "--2025-08-26 23:16:57--  https://github.com/D2KLab/Face-Celebrity-Recognition/raw/master/model/facenet_keras.h5\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://github.com/D2KLab/FaceRec/raw/master/model/facenet_keras.h5 [following]\n",
            "--2025-08-26 23:16:57--  https://github.com/D2KLab/FaceRec/raw/master/model/facenet_keras.h5\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/D2KLab/FaceRec/master/model/facenet_keras.h5 [following]\n",
            "--2025-08-26 23:16:57--  https://raw.githubusercontent.com/D2KLab/FaceRec/master/model/facenet_keras.h5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 92397640 (88M) [application/octet-stream]\n",
            "Saving to: ‘/content/gdrive/My Drive/facial_recognition/output/facenet_keras.h5’\n",
            "\n",
            "/content/gdrive/My  100%[===================>]  88.12M   402MB/s    in 0.2s    \n",
            "\n",
            "2025-08-26 23:16:57 (402 MB/s) - ‘/content/gdrive/My Drive/facial_recognition/output/facenet_keras.h5’ saved [92397640/92397640]\n",
            "\n",
            "\n",
            "--- Verificação Pós-Download ---\n",
            "Verificando o tamanho do arquivo...\n",
            "-rw-r--r-- 1 root root 89M Aug 26 23:16 '/content/gdrive/My Drive/facial_recognition/output/facenet_keras.h5'\n",
            "\n",
            "Verificando o tipo do arquivo...\n",
            "/content/gdrive/My Drive/facial_recognition/output/facenet_keras.h5: Hierarchical Data Format (version 5) data\n",
            "\n",
            "DOWNLOAD BEM-SUCEDIDO! Se o tamanho for ~95M e o tipo for 'HDF data', você pode prosseguir para a Célula 4.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ETAPA 4: PROCESSAMENTO DO DATASET E GERAÇÃO DE EMBEDDINGS (VERSÃO MELHORADA) ---\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Define o caminho para o modelo e VERIFICA SE ELE EXISTE\n",
        "facenet_model_path = os.path.join(output_dir, 'facenet_keras.h5')\n",
        "if not os.path.exists(facenet_model_path):\n",
        "    print(f\"ERRO CRÍTICO: O arquivo do modelo FaceNet não foi encontrado em '{facenet_model_path}'\")\n",
        "    print(\"Por favor, execute a Célula 3 para fazer o download do modelo antes de continuar.\")\n",
        "else:\n",
        "    print(\"Iniciando a geração de embeddings...\")\n",
        "\n",
        "    # Carrega os modelos\n",
        "    detector = MTCNN()\n",
        "    facenet_model = load_model(facenet_model_path)\n",
        "    print(\"Modelos MTCNN e FaceNet carregados.\")\n",
        "\n",
        "    def get_embedding(face_pixels):\n",
        "        # Padroniza os valores dos pixels\n",
        "        face_pixels = face_pixels.astype('float32')\n",
        "        mean, std = face_pixels.mean(), face_pixels.std()\n",
        "        face_pixels = (face_pixels - mean) / std\n",
        "\n",
        "        # Adiciona uma dimensão para o lote (batch)\n",
        "        samples = np.expand_dims(face_pixels, axis=0)\n",
        "\n",
        "        # Gera o embedding\n",
        "        yhat = facenet_model.predict(samples)\n",
        "        return yhat[0]\n",
        "\n",
        "    # Listas para armazenar os embeddings e os rótulos\n",
        "    embeddings_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    # Itera sobre as pastas de cada pessoa no dataset\n",
        "    for person_name in os.listdir(dataset_dir):\n",
        "        person_dir = os.path.join(dataset_dir, person_name)\n",
        "        if not os.path.isdir(person_dir):\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nProcessando imagens de: {person_name}\")\n",
        "        for image_name in os.listdir(person_dir):\n",
        "            image_path = os.path.join(person_dir, image_name)\n",
        "            image = cv2.imread(image_path)\n",
        "\n",
        "            # Converte para RGB, pois MTCNN espera RGB\n",
        "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Detecta faces\n",
        "            results = detector.detect_faces(image_rgb)\n",
        "\n",
        "            # Extrai a face (assume uma face por imagem para o dataset)\n",
        "            if results:\n",
        "                x1, y1, width, height = results[0]['box']\n",
        "                x1, y1 = abs(x1), abs(y1)\n",
        "                x2, y2 = x1 + width, y1 + height\n",
        "\n",
        "                face = image_rgb[y1:y2, x1:x2]\n",
        "\n",
        "                # Redimensiona para o tamanho esperado pelo FaceNet\n",
        "                face_resized = cv2.resize(face, (160, 160))\n",
        "\n",
        "                # Gera e armazena o embedding\n",
        "                embedding = get_embedding(face_resized)\n",
        "                embeddings_list.append(embedding)\n",
        "                labels_list.append(person_name)\n",
        "                print(f\"> {image_name}\")\n",
        "\n",
        "    # Salva os embeddings e rótulos\n",
        "    embeddings_file = os.path.join(output_dir, 'embeddings.npz')\n",
        "    np.savez_compressed(embeddings_file, embeddings=np.asarray(embeddings_list), labels=np.asarray(labels_list))\n",
        "\n",
        "    print(f\"\\nProcesso concluído! Embeddings e rótulos salvos em '{embeddings_file}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "tesMAhpjb-LF",
        "outputId": "0db56092-ab21-4092-b256-29657ce34bea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando a geração de embeddings...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "EOFError",
          "evalue": "EOF read where object expected",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-529122171.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Carrega os modelos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMTCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mfacenet_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfacenet_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Modelos MTCNN e FaceNet carregados.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    194\u001b[0m         )\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         return legacy_h5_format.load_model_from_hdf5(\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msaving_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_option_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_legacy_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             model = saving_utils.model_from_config(\n\u001b[0m\u001b[1;32m    134\u001b[0m                 \u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/legacy/saving/saving_utils.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_replace_nested_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     return serialization.deserialize_keras_object(\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODULE_OBJECTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/legacy/saving/serialization.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"custom_objects\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m                 deserialized_obj = cls.from_config(\n\u001b[0m\u001b[1;32m    496\u001b[0m                     \u001b[0mcls_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m                     custom_objects={\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/models/model.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional_from_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             return functional_from_config(\n\u001b[0m\u001b[1;32m    652\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mfunctional_from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunctional_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"layers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0;31m# Legacy format deserialization (no \"module\" key)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0;31m# used for H5 and SavedModel formats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             layer = saving_utils.model_from_config(\n\u001b[0m\u001b[1;32m    524\u001b[0m                 \u001b[0mlayer_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/legacy/saving/saving_utils.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_replace_nested_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     return serialization.deserialize_keras_object(\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODULE_OBJECTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/legacy/saving/serialization.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"custom_objects\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m                 deserialized_obj = cls.from_config(\n\u001b[0m\u001b[1;32m    496\u001b[0m                     \u001b[0mcls_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m                     custom_objects={\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/lambda_layer.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects, safe_mode)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_for_lambda_deserialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"function\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0minner_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             fn = python_utils.func_load(\n\u001b[0m\u001b[1;32m    193\u001b[0m                 \u001b[0minner_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"code\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mdefaults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"defaults\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/python_utils.py\u001b[0m in \u001b[0;36mfunc_load\u001b[0;34m(code, defaults, closure, globs)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mUnicodeEncodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinascii\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mraw_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"raw_unicode_escape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarshal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mglobs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mglobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEOFError\u001b[0m: EOF read where object expected"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ETAPA 5: TREINAMENTO DO CLASSIFICADOR SVM ---\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "print(\"Iniciando o treinamento do classificador...\")\n",
        "\n",
        "# Carrega os embeddings salvos\n",
        "embeddings_file = os.path.join(output_dir, 'embeddings.npz')\n",
        "data = np.load(embeddings_file)\n",
        "embeddings, labels = data['embeddings'], data['labels']\n",
        "\n",
        "# Codifica os rótulos (nomes) em números\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Treina o modelo SVM\n",
        "svm_model = SVC(kernel='linear', probability=True)\n",
        "svm_model.fit(embeddings, encoded_labels)\n",
        "print(\"Treinamento do SVM concluído.\")\n",
        "\n",
        "# Salva o modelo treinado e o codificador de rótulos\n",
        "svm_model_path = os.path.join(output_dir, 'svm_model.pkl')\n",
        "label_encoder_path = os.path.join(output_dir, 'label_encoder.pkl')\n",
        "\n",
        "with open(svm_model_path, 'wb') as f:\n",
        "    pickle.dump(svm_model, f)\n",
        "with open(label_encoder_path, 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "print(f\"Modelo SVM salvo em: '{svm_model_path}'\")\n",
        "print(f\"Codificador de rótulos salvo em: '{label_encoder_path}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "NUsMIuOccGeW",
        "outputId": "f9f24fce-c0b7-4407-f3be-7892952aa68b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando o treinamento do classificador...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/gdrive/My Drive/facial_recognition/output/embeddings.npz'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1556974117.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Carrega os embeddings salvos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0membeddings_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'embeddings.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embeddings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/facial_recognition/output/embeddings.npz'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ETAPA 6: APLICAÇÃO DE RECONHECIMENTO FACIAL ---\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pickle\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "from tensorflow.keras.models import load_model\n",
        "from google.colab import files\n",
        "from IPython.display import display, Image\n",
        "\n",
        "# Carrega todos os modelos e arquivos necessários\n",
        "print(\"Carregando modelos e dados...\")\n",
        "detector = MTCNN()\n",
        "facenet_model = load_model(os.path.join(output_dir, 'facenet_keras.h5'))\n",
        "svm_model = pickle.load(open(os.path.join(output_dir, 'svm_model.pkl'), 'rb'))\n",
        "label_encoder = pickle.load(open(os.path.join(output_dir, 'label_encoder.pkl'), 'rb'))\n",
        "print(\"Pronto para o reconhecimento.\")\n",
        "\n",
        "# Função para fazer o upload da imagem\n",
        "uploaded = files.upload()\n",
        "test_image_path = list(uploaded.keys())[0]\n",
        "\n",
        "# Carrega e processa a imagem de teste\n",
        "image = cv2.imread(test_image_path)\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "results = detector.detect_faces(image_rgb)\n",
        "\n",
        "# Itera sobre todas as faces detectadas na imagem\n",
        "for result in results:\n",
        "    x1, y1, width, height = result['box']\n",
        "    x1, y1 = abs(x1), abs(y1)\n",
        "    x2, y2 = x1 + width, y1 + height\n",
        "\n",
        "    face = image_rgb[y1:y2, x1:x2]\n",
        "    face_resized = cv2.resize(face, (160, 160))\n",
        "\n",
        "    # Gera o embedding da face detectada\n",
        "    embedding = get_embedding(face_resized)\n",
        "    embedding = np.expand_dims(embedding, axis=0)\n",
        "\n",
        "    # Faz a predição usando o SVM\n",
        "    prediction = svm_model.predict(embedding)\n",
        "    proba = svm_model.predict_proba(embedding)\n",
        "    confidence = np.max(proba)\n",
        "\n",
        "    # Decodifica o rótulo para obter o nome\n",
        "    predicted_label = label_encoder.inverse_transform(prediction)[0]\n",
        "\n",
        "    # Define um limiar de confiança\n",
        "    if confidence > 0.75: # Limiar de 75% de confiança\n",
        "        text = f\"{predicted_label} ({confidence:.2f})\"\n",
        "    else:\n",
        "        text = \"Desconhecido\"\n",
        "\n",
        "    # Desenha o retângulo e o texto na imagem original\n",
        "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "    cv2.putText(image, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "# Salva e exibe a imagem resultante\n",
        "output_image_path = '/content/output_image.jpg'\n",
        "cv2.imwrite(output_image_path, image)\n",
        "\n",
        "print(\"\\n--- Resultado do Reconhecimento ---\")\n",
        "display(Image(output_image_path))"
      ],
      "metadata": {
        "id": "0e5FSNuscIU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f8dN-OTxcKb1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}